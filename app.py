import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from datetime import datetime, timedelta
import re

# Seitenkonfiguration
st.set_page_config(
    page_title="Amazon Business Report Analyzer",
    page_icon="ðŸ“Š",
    layout="wide"
)

# Titel
st.title("ðŸ“Š Amazon Business Report Analyzer")
st.markdown("Analysiere deine Amazon Business Reports fÃ¼r Detailseite VerkÃ¤ufe und Traffic")

# Hilfsfunktionen
def parse_euro_value(value):
    """Konvertiert Euro-Strings (z.B. '1.999,55 â‚¬' oder '368,14 â‚¬') zu Float"""
    if pd.isna(value) or value == '':
        return 0.0
    if isinstance(value, (int, float)):
        return float(value)
    
    # Entferne Leerzeichen und â‚¬
    value_str = str(value).replace(' ', '').replace('â‚¬', '').strip()
    
    # Format: "1.999,55" (Punkt = Tausender, Komma = Dezimal)
    # PrÃ¼fe ob Punkt als Tausendertrennzeichen verwendet wird (mehr als ein Punkt)
    if '.' in value_str and ',' in value_str:
        # Format: "1.999,55" - Punkt ist Tausender, Komma ist Dezimal
        value_str = value_str.replace('.', '').replace(',', '.')
    elif ',' in value_str:
        # Format: "368,14" - Komma ist Dezimal
        value_str = value_str.replace(',', '.')
    # Falls nur Punkt vorhanden, kÃ¶nnte es Tausender oder Dezimal sein
    # Wenn mehr als ein Punkt, dann Tausender
    elif value_str.count('.') > 1:
        value_str = value_str.replace('.', '')
    
    try:
        return float(value_str)
    except:
        return 0.0

def parse_percentage(value):
    """Konvertiert Prozent-Strings (z.B. '16,40%' oder '16.40%') zu Float"""
    if pd.isna(value) or value == '':
        return 0.0
    if isinstance(value, (int, float)):
        return float(value)
    
    value_str = str(value).replace('%', '').replace(' ', '').strip()
    
    # Komma als Dezimaltrennzeichen (deutsches Format)
    if ',' in value_str:
        value_str = value_str.replace(',', '.')
    
    try:
        return float(value_str)
    except:
        return 0.0

def parse_numeric_value(value):
    """Konvertiert numerische Strings mit deutschem Format (z.B. '9,778' oder '6,333') zu Float"""
    if pd.isna(value) or value == '':
        return 0.0
    if isinstance(value, (int, float)):
        return float(value)
    
    value_str = str(value).replace(' ', '').strip()
    
    # Format: "9,778" (Komma als Tausendertrennzeichen) oder "1.234,56" (Punkt = Tausender, Komma = Dezimal)
    if '.' in value_str and ',' in value_str:
        # Format: "1.234,56" - Punkt ist Tausender, Komma ist Dezimal
        value_str = value_str.replace('.', '').replace(',', '.')
    elif ',' in value_str:
        # PrÃ¼fe ob Komma Tausender oder Dezimal ist
        parts = value_str.split(',')
        if len(parts) == 2 and len(parts[1]) <= 2:
            # Komma ist Dezimaltrennzeichen (z.B. "123,45")
            value_str = value_str.replace(',', '.')
        else:
            # Komma ist Tausendertrennzeichen (z.B. "9,778" oder "6,333")
            value_str = value_str.replace(',', '')
    # Falls nur Punkt vorhanden und mehr als einer, dann Tausender
    elif value_str.count('.') > 1:
        value_str = value_str.replace('.', '')
    
    try:
        return float(value_str)
    except:
        return 0.0

def parse_date_column(date_str):
    """Parst Datum im Format DD.MM.YY zu YYYY-MM-DD"""
    if pd.isna(date_str) or date_str == '':
        return None
    date_str = str(date_str).strip()
    # Versuche verschiedene Formate
    date_match = re.search(r'(\d{2})\.(\d{2})\.(\d{2})', date_str)
    if date_match:
        day, month, year = date_match.groups()
        year_full = f"20{year}" if int(year) < 50 else f"19{year}"
        return f"{year_full}-{month}-{day}"
    return date_str

def load_and_process_csv(uploaded_file, file_name):
    """LÃ¤dt und verarbeitet eine CSV-Datei (ASIN-Level oder Account-Level)"""
    try:
        df = pd.read_csv(uploaded_file, encoding='utf-8')
        
        # Entferne doppelte Spaltennamen (behalte die erste)
        if df.columns.duplicated().any():
            df = df.loc[:, ~df.columns.duplicated()]
        
        # PrÃ¼fe ob es ein Account-Level Report ist (hat "Datum"-Spalte)
        is_account_level = 'Datum' in df.columns
        
        if is_account_level:
            # Account-Level Report: Verwende Datumsspalte
            df['Zeitraum'] = df['Datum'].apply(parse_date_column)
            df = df.dropna(subset=['Zeitraum'])  # Entferne Zeilen ohne gÃ¼ltiges Datum
            df['Dateiname'] = file_name
            df['Report_Typ'] = 'Account-Level'
        else:
            # ASIN-Level Report: Extrahiere Datum aus Dateinamen
            date_match = re.search(r'(\d{2})\.(\d{2})\.(\d{2})', file_name)
            if date_match:
                day, month, year = date_match.groups()
                year_full = f"20{year}" if int(year) < 50 else f"19{year}"
                date_str = f"{year_full}-{month}-{day}"
            else:
                date_str = file_name
            
            df['Zeitraum'] = date_str
            df['Dateiname'] = file_name
            df['Report_Typ'] = 'ASIN-Level'
        
        # Verarbeite numerische Spalten
        numeric_columns = [
            'Bestellte Einheiten',
            'Bestellte Einheiten â€“ B2B',
            'Durch bestellte Produkte erzielter Umsatz',
            'Bestellsumme â€“ B2B',
            'Seitenaufrufe â€“ Summe',
            'Seitenaufrufe â€“ Summe â€“ B2B',
            'Sitzungen â€“ Summe',
            'Sitzungen â€“ Summe â€“ B2B',
            'Zahl der Bestellposten',
            'Zahl der Bestellposten â€“ B2B',
            'Sitzungen â€“ mobile App',
            'Sitzungen â€“ mobile App â€“ B2B',
            'Sitzungen â€“ Browser',
            'Sitzungen â€“ Browser â€“ B2B',
            # ZusÃ¤tzliche Spalten
            'Durchschnittlicher Umsatz/Bestellposten',
            'Durchschnittlicher Umsatz pro Bestellposten â€“ B2B',
            'Durchschnitt Anzahl von Einheiten/Bestellposten',
            'Durchschnitt Anzahl von Einheiten/Bestellposten â€“ B2B',
            'Durchschnittlicher Verkaufspreis',
            'Durchschnittlicher Verkaufspreis â€“ B2B',
            'Prozentsatz Bestellposten pro Sitzung',
            'Bestellposten pro Sitzung Prozentwert â€“ B2B',
            'Durchschnittliche Angebotszahl'
        ]
        
        for col in numeric_columns:
            if col in df.columns:
                # Euro-Werte
                if 'Umsatz' in col or 'Bestellsumme' in col or 'Verkaufspreis' in col:
                    df[col] = df[col].apply(parse_euro_value)
                # Prozentwerte
                elif 'Prozentsatz' in col or 'Prozentwert' in col or col.endswith('%'):
                    df[col] = df[col].apply(parse_percentage)
                # Normale numerische Werte (kÃ¶nnen auch mit Komma als Tausendertrennzeichen sein)
                else:
                    # Konvertiere zu String, dann parse mit deutschem Format
                    df[col] = df[col].apply(parse_numeric_value)
        
        return df
    except Exception as e:
        st.error(f"Fehler beim Laden der Datei {file_name}: {str(e)}")
        return None

def find_column(df, possible_names):
    """Findet eine Spalte anhand mehrerer mÃ¶glicher Namen"""
    # Zuerst exakte Ãœbereinstimmung versuchen
    for name in possible_names:
        if name in df.columns:
            return name
    
    # Falls keine exakte Ãœbereinstimmung, suche nach Ã¤hnlichen Namen (normalisiert)
    # Normalisiere alle Spaltennamen und Suchbegriffe
    normalized_columns = {col.strip().replace('â€“', '-').replace('â€”', '-').replace(' ', '').lower(): col for col in df.columns}
    
    for name in possible_names:
        normalized_name = name.strip().replace('â€“', '-').replace('â€”', '-').replace(' ', '').lower()
        if normalized_name in normalized_columns:
            return normalized_columns[normalized_name]
    
    # ZusÃ¤tzliche Suche: Teilstring-Matching
    for name in possible_names:
        name_keywords = name.lower().split()
        for col in df.columns:
            col_lower = col.lower()
            # PrÃ¼fe ob alle wichtigen Keywords in Spaltenname enthalten sind
            if all(keyword in col_lower for keyword in name_keywords if len(keyword) > 2):
                return col
    
    return None

def aggregate_data(df, traffic_type='normal', is_account_level=False):
    """Aggregiert Daten Ã¼ber alle ASINs (oder Account-Level) und berechnet zusÃ¤tzliche KPIs"""
    if traffic_type == 'B2B':
        units_col = find_column(df, ['Bestellte Einheiten â€“ B2B', 'Bestellte Einheiten - B2B'])
        revenue_col = find_column(df, ['Bestellsumme â€“ B2B', 'Bestellsumme - B2B'])
        views_col = find_column(df, [
            'Seitenaufrufe â€“ Summe â€“ B2B',
            'Seitenaufrufe - Summe - B2B',
            'Sitzungen â€“ Summe â€“ B2B',
            'Sitzungen - Summe - B2B'
        ])
        sessions_col = find_column(df, ['Sitzungen â€“ Summe â€“ B2B', 'Sitzungen - Summe - B2B'])
        orders_col = find_column(df, ['Zahl der Bestellposten â€“ B2B', 'Zahl der Bestellposten - B2B'])
        mobile_sessions_col = find_column(df, ['Sitzungen â€“ mobile App â€“ B2B', 'Sitzungen - mobile App - B2B'])
        browser_sessions_col = find_column(df, ['Sitzungen â€“ Browser â€“ B2B', 'Sitzungen - Browser - B2B'])
    else:
        units_col = find_column(df, ['Bestellte Einheiten'])
        revenue_col = find_column(df, ['Durch bestellte Produkte erzielter Umsatz'])
        # Die korrekte Spalte heiÃŸt "Seitenaufrufe â€“ Summe"
        views_col = find_column(df, [
            'Seitenaufrufe â€“ Summe',
            'Seitenaufrufe - Summe',
            'Sitzungen â€“ Summe',
            'Sitzungen - Summe'
        ])
        sessions_col = find_column(df, ['Sitzungen â€“ Summe', 'Sitzungen - Summe'])
        orders_col = find_column(df, ['Zahl der Bestellposten'])
        mobile_sessions_col = find_column(df, ['Sitzungen â€“ mobile App', 'Sitzungen - mobile App'])
        browser_sessions_col = find_column(df, ['Sitzungen â€“ Browser', 'Sitzungen - Browser'])
    
    # PrÃ¼fe ob alle benÃ¶tigten Spalten vorhanden sind
    # WICHTIG: PrÃ¼fe ob Spalte wirklich im DataFrame existiert, nicht ob Werte 0 sind
    missing_cols = []
    
    # FÃ¼r units_col - prÃ¼fe ob Spalte existiert, auch wenn find_column None zurÃ¼ckgab
    if units_col is None:
        expected_name = 'Bestellte Einheiten' + (' â€“ B2B' if traffic_type == 'B2B' else '')
        # PrÃ¼fe ob Spalte trotzdem existiert (mit exaktem Namen)
        if expected_name in df.columns:
            units_col = expected_name
        else:
            # Spalte fehlt wirklich
            missing_cols.append(expected_name)
            df[expected_name] = 0
            units_col = expected_name
    
    # FÃ¼r revenue_col
    if revenue_col is None:
        expected_name = 'Bestellsumme â€“ B2B' if traffic_type == 'B2B' else 'Durch bestellte Produkte erzielter Umsatz'
        if expected_name in df.columns:
            revenue_col = expected_name
        else:
            missing_cols.append(expected_name)
            df[expected_name] = 0
            revenue_col = expected_name
    
    # FÃ¼r views_col - erweiterte Suche
    if views_col is None:
        expected_name = 'Seitenaufrufe â€“ Summe' + (' â€“ B2B' if traffic_type == 'B2B' else '')
        # PrÃ¼fe ob Spalte trotzdem existiert (mit exaktem Namen)
        if expected_name in df.columns:
            views_col = expected_name
        else:
            # Suche nach Spalten die "Seitenaufrufe" oder "Sitzungen" und "Summe" enthalten
            search_keywords = ['seitenaufrufe', 'summe'] if traffic_type != 'B2B' else ['seitenaufrufe', 'summe', 'b2b']
            matching_cols = []
            for col in df.columns:
                col_lower = col.lower()
                if all(keyword in col_lower for keyword in search_keywords):
                    matching_cols.append(col)
            
            if matching_cols:
                # Nimm die erste passende Spalte
                views_col = matching_cols[0]
            else:
                missing_cols.append(expected_name)
                df[expected_name] = 0
                views_col = expected_name
    
    # FÃ¼r sessions_col
    if sessions_col is None:
        expected_name = 'Sitzungen â€“ Summe' + (' â€“ B2B' if traffic_type == 'B2B' else '')
        if expected_name in df.columns:
            sessions_col = expected_name
        else:
            df[expected_name] = 0
            sessions_col = expected_name
    
    # FÃ¼r orders_col
    if orders_col is None:
        expected_name = 'Zahl der Bestellposten' + (' â€“ B2B' if traffic_type == 'B2B' else '')
        if expected_name in df.columns:
            orders_col = expected_name
        else:
            df[expected_name] = 0
            orders_col = expected_name
    
    # FÃ¼r mobile_sessions_col
    if mobile_sessions_col is None:
        expected_name = 'Sitzungen â€“ mobile App' + (' â€“ B2B' if traffic_type == 'B2B' else '')
        if expected_name in df.columns:
            mobile_sessions_col = expected_name
        else:
            df[expected_name] = 0
            mobile_sessions_col = expected_name
    
    # FÃ¼r browser_sessions_col
    if browser_sessions_col is None:
        expected_name = 'Sitzungen â€“ Browser' + (' â€“ B2B' if traffic_type == 'B2B' else '')
        if expected_name in df.columns:
            browser_sessions_col = expected_name
        else:
            df[expected_name] = 0
            browser_sessions_col = expected_name
    
    # DEBUG: Zeige welche Spalten gefunden wurden
    debug_info = []
    debug_info.append(f"**Gefundene Spalten fÃ¼r {traffic_type} Traffic:**")
    debug_info.append(f"- Bestellte Einheiten: {units_col if units_col else 'NICHT GEFUNDEN'}")
    debug_info.append(f"- Umsatz: {revenue_col if revenue_col else 'NICHT GEFUNDEN'}")
    debug_info.append(f"- Seitenaufrufe: {views_col if views_col else 'NICHT GEFUNDEN'}")
    
    # PrÃ¼fe ob Spalten wirklich im DataFrame existieren
    final_missing = []
    if units_col and units_col not in df.columns:
        final_missing.append(units_col)
    if revenue_col and revenue_col not in df.columns:
        final_missing.append(revenue_col)
    if views_col and views_col not in df.columns:
        final_missing.append(views_col)
    
    # Zeige Debug-Info in einem Expander
    with st.expander("ðŸ” Debug: Spaltensuche", expanded=False):
        st.markdown("\n".join(debug_info))
        if final_missing:
            st.error(f"âš ï¸ Diese Spalten wurden nicht im DataFrame gefunden: {', '.join(final_missing)}")
        else:
            st.success("âœ… Alle benÃ¶tigten Spalten wurden gefunden!")
    
    if final_missing:
        st.warning(f"âš ï¸ Folgende Spalten fehlen wirklich in den Daten: {', '.join(final_missing)}")
    
    # Bei Account-Level Reports sind die Daten bereits aggregiert, bei ASIN-Level mÃ¼ssen wir gruppieren
    if is_account_level:
        # Daten sind bereits pro Zeitraum aggregiert
        aggregated = df.copy()
        # Stelle sicher, dass keine doppelten Spaltennamen existieren
        if aggregated.columns.duplicated().any():
            aggregated = aggregated.loc[:, ~aggregated.columns.duplicated()]
        # Stelle sicher, dass alle benÃ¶tigten Spalten vorhanden sind
        for col in [units_col, revenue_col, views_col, sessions_col, orders_col, mobile_sessions_col, browser_sessions_col]:
            if col not in aggregated.columns:
                aggregated[col] = 0
    else:
        # ASIN-Level: Gruppiere nach Zeitraum
        aggregated = df.groupby('Zeitraum').agg({
            units_col: 'sum',
            revenue_col: 'sum',
            views_col: 'sum',
            sessions_col: 'sum',
            orders_col: 'sum',
            mobile_sessions_col: 'sum',
            browser_sessions_col: 'sum'
        }).reset_index()
    
    # Stelle sicher, dass alle Spalten numerisch sind (mit deutschem Format)
    for col in [units_col, revenue_col, views_col, sessions_col, orders_col, mobile_sessions_col, browser_sessions_col]:
        if col in aggregated.columns:
            # Verwende parse_numeric_value fÃ¼r alle numerischen Werte (erkennt Komma als Tausender)
            # Ausnahme: revenue_col verwendet parse_euro_value
            if col == revenue_col:
                aggregated[col] = aggregated[col].apply(parse_euro_value)
            else:
                aggregated[col] = aggregated[col].apply(parse_numeric_value)
    
    # Berechne zusÃ¤tzliche KPIs (mit Division durch Null Schutz)
    # Spalten sind bereits numerisch konvertiert, kÃ¶nnen direkt verwendet werden
    aggregated['Conversion Rate (%)'] = (
        (aggregated[units_col] / aggregated[sessions_col].replace(0, np.nan) * 100)
        .fillna(0)
        .replace([np.inf, -np.inf], 0)
    )
    
    # AOV = Umsatz / Anzahl der Bestellposten
    # PrÃ¼fe zuerst, ob bereits eine AOV-Spalte in den Originaldaten vorhanden ist
    aov_col_name = 'Durchschnittlicher Umsatz/Bestellposten' if traffic_type == 'normal' else 'Durchschnittlicher Umsatz pro Bestellposten â€“ B2B'
    aov_col_alt = find_column(df, [aov_col_name, 'Durchschnittlicher Umsatz/Bestellposten', 'Durchschnittlicher Umsatz pro Bestellposten â€“ B2B'])
    
    if aov_col_alt and aov_col_alt in df.columns:
        # Wenn AOV-Spalte in Originaldaten vorhanden ist, verwende diese
        # Aggregiere die AOV-Werte (gewichtet nach Anzahl der Bestellposten)
        if is_account_level:
            # Bei Account-Level: AOV ist bereits pro Zeitraum vorhanden
            if aov_col_alt in aggregated.columns:
                aggregated['AOV (â‚¬)'] = aggregated[aov_col_alt]
            else:
                # Fallback: Berechne aus Umsatz / Bestellposten
                aggregated['AOV (â‚¬)'] = (
                    (aggregated[revenue_col] / aggregated[orders_col].replace(0, np.nan))
                    .fillna(0)
                    .replace([np.inf, -np.inf], 0)
                )
        else:
            # Bei ASIN-Level: Gewichteter Durchschnitt der AOV-Werte
            # AOV gesamt = Summe(Umsatz) / Summe(Bestellposten)
            aggregated['AOV (â‚¬)'] = (
                (aggregated[revenue_col] / aggregated[orders_col].replace(0, np.nan))
                .fillna(0)
                .replace([np.inf, -np.inf], 0)
            )
    else:
        # Berechne AOV aus Umsatz / Anzahl der Bestellposten
        aggregated['AOV (â‚¬)'] = (
            (aggregated[revenue_col] / aggregated[orders_col].replace(0, np.nan))
            .fillna(0)
            .replace([np.inf, -np.inf], 0)
        )
    
    # Debug: Zeige welche Spalten fÃ¼r AOV verwendet werden
    with st.expander("ðŸ” Debug: AOV-Berechnung", expanded=False):
        st.write(f"**Verwendete Spalten fÃ¼r AOV:**")
        st.write(f"- Umsatz-Spalte: `{revenue_col}`")
        st.write(f"- Bestellposten-Spalte: `{orders_col}`")
        if aov_col_alt:
            st.write(f"- Gefundene AOV-Spalte in Originaldaten: `{aov_col_alt}` (wird nicht verwendet, da gewichteter Durchschnitt benÃ¶tigt wird)")
        if orders_col in aggregated.columns:
            st.write(f"- Beispielwerte aus `{orders_col}`: {aggregated[orders_col].head(3).tolist()}")
        if revenue_col in aggregated.columns:
            st.write(f"- Beispielwerte aus `{revenue_col}`: {aggregated[revenue_col].head(3).tolist()}")
        if 'AOV (â‚¬)' in aggregated.columns:
            st.write(f"- Berechnete AOV-Werte: {aggregated['AOV (â‚¬)'].head(3).tolist()}")
            if len(aggregated) > 0:
                sample_idx = 0
                if orders_col in aggregated.columns and aggregated[orders_col].iloc[sample_idx] != 0:
                    manual_calc = aggregated[revenue_col].iloc[sample_idx] / aggregated[orders_col].iloc[sample_idx]
                    st.write(f"- Manuelle PrÃ¼fung (Zeile 0): {aggregated[revenue_col].iloc[sample_idx]:.2f} â‚¬ / {aggregated[orders_col].iloc[sample_idx]:.0f} = {manual_calc:.2f} â‚¬")
                    st.write(f"- TatsÃ¤chlicher berechneter Wert: {aggregated['AOV (â‚¬)'].iloc[sample_idx]:.2f} â‚¬")
    aggregated['Revenue per Session (â‚¬)'] = (
        (aggregated[revenue_col] / aggregated[sessions_col].replace(0, np.nan))
        .fillna(0)
        .replace([np.inf, -np.inf], 0)
    )
    
    # Umbenennen der Spalten - nur die Spalten die tatsÃ¤chlich vorhanden sind
    # Erstelle Mapping ohne 'Zeitraum' (wird nicht umbenannt)
    column_mapping = {
        units_col: 'Bestellte Einheiten',
        revenue_col: 'Umsatz',
        views_col: 'Seitenaufrufe',
        sessions_col: 'Sitzungen',
        orders_col: 'Bestellungen',
        mobile_sessions_col: 'Mobile Sitzungen',
        browser_sessions_col: 'Browser Sitzungen'
    }
    
    # PrÃ¼fe auf doppelte Zielnamen und benenne nur um, wenn nÃ¶tig
    rename_dict = {}
    for old_name, new_name in column_mapping.items():
        if old_name in aggregated.columns and old_name != new_name:
            # PrÃ¼fe ob Zielname bereits existiert (aber nicht als die aktuelle Spalte)
            if new_name not in aggregated.columns or aggregated.columns.get_loc(new_name) != aggregated.columns.get_loc(old_name):
                rename_dict[old_name] = new_name
    
    # FÃ¼hre Umbenennung in einem Schritt durch
    if rename_dict:
        aggregated = aggregated.rename(columns=rename_dict)
    
    # Stelle sicher, dass keine doppelten Spaltennamen existieren
    if aggregated.columns.duplicated().any():
        # Entferne doppelte Spalten (behalte die erste)
        aggregated = aggregated.loc[:, ~aggregated.columns.duplicated()]
    
    return aggregated

def aggregate_by_period(df, period='week'):
    """Aggregiert Daten nach Zeitraum (Woche, Monat, YTD)"""
    if 'Zeitraum' not in df.columns:
        return df
    
    # Konvertiere Zeitraum zu Datetime
    df = df.copy()
    df['Zeitraum_DT'] = pd.to_datetime(df['Zeitraum'], errors='coerce')
    df = df.dropna(subset=['Zeitraum_DT'])
    
    if len(df) == 0:
        return df
    
    if period == 'week':
        # Aggregiere nach Woche (Jahr-Kalenderwoche)
        df['Zeitraum_Agg'] = df['Zeitraum_DT'].dt.to_period('W').astype(str)
    elif period == 'month':
        # Aggregiere nach Monat (Jahr-Monat)
        df['Zeitraum_Agg'] = df['Zeitraum_DT'].dt.to_period('M').astype(str)
    elif period == 'ytd':
        # Year-to-Date: Gruppiere nach Jahr
        df['Jahr'] = df['Zeitraum_DT'].dt.year
        df['Zeitraum_Agg'] = df['Jahr'].astype(str) + ' (YTD)'
    else:
        # Fallback: Keine Aggregation (sollte nicht vorkommen, da Tag entfernt wurde)
        df['Zeitraum_Agg'] = df['Zeitraum_DT'].dt.strftime('%Y-%m-%d')
    
    # Identifiziere Spalten die NICHT summiert werden sollen (sondern neu berechnet)
    # AOV und Conversion Rate mÃ¼ssen neu berechnet werden, nicht summiert
    exclude_from_sum = ['AOV (â‚¬)', 'Conversion Rate (%)', 'Revenue per Session (â‚¬)', 'Zeitraum_DT', 'Zeitraum_Nr']
    if 'Jahr' in df.columns:
        exclude_from_sum.append('Jahr')
    
    # Numerische Spalten fÃ¼r Aggregation identifizieren
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    # Entferne Spalten die nicht summiert werden sollen
    numeric_cols = [col for col in numeric_cols if col not in exclude_from_sum]
    
    # Gruppiere und aggregiere (nur Spalten die summiert werden sollen)
    agg_dict = {col: 'sum' for col in numeric_cols if col in df.columns}
    agg_dict['Zeitraum_DT'] = 'first'  # Behalte erstes Datum fÃ¼r Sortierung
    
    aggregated = df.groupby('Zeitraum_Agg', as_index=False).agg(agg_dict)
    
    # Sortiere nach Datum
    aggregated = aggregated.sort_values('Zeitraum_DT')
    aggregated['Zeitraum'] = aggregated['Zeitraum_Agg']
    aggregated = aggregated.drop(columns=['Zeitraum_DT', 'Zeitraum_Agg'])
    
    # Entferne temporÃ¤re Spalten
    if 'Zeitraum_DT' in aggregated.columns:
        aggregated = aggregated.drop(columns=['Zeitraum_DT'])
    if 'Zeitraum_Agg' in aggregated.columns:
        aggregated = aggregated.drop(columns=['Zeitraum_Agg'])
    
    # Berechne AOV, Conversion Rate und Revenue per Session NEU fÃ¼r aggregierte ZeitrÃ¤ume
    # Diese mÃ¼ssen aus den aggregierten Basiswerten neu berechnet werden, nicht summiert werden
    
    # Finde die Basis-Spalten fÃ¼r die Berechnung
    # Diese sollten bereits in aggregated vorhanden sein (wurden summiert)
    units_col_agg = 'Bestellte Einheiten' if 'Bestellte Einheiten' in aggregated.columns else None
    revenue_col_agg = 'Umsatz' if 'Umsatz' in aggregated.columns else None
    sessions_col_agg = 'Sitzungen' if 'Sitzungen' in aggregated.columns else None
    orders_col_agg = 'Bestellungen' if 'Bestellungen' in aggregated.columns else None
    
    # Conversion Rate = (Bestellte Einheiten / Sitzungen) * 100
    if units_col_agg and sessions_col_agg:
        aggregated['Conversion Rate (%)'] = (
            (aggregated[units_col_agg] / aggregated[sessions_col_agg].replace(0, np.nan) * 100)
            .fillna(0)
            .replace([np.inf, -np.inf], 0)
        )
    
    # AOV = Umsatz / Anzahl der Bestellposten
    if revenue_col_agg and orders_col_agg:
        aggregated['AOV (â‚¬)'] = (
            (aggregated[revenue_col_agg] / aggregated[orders_col_agg].replace(0, np.nan))
            .fillna(0)
            .replace([np.inf, -np.inf], 0)
        )
    
    # Revenue per Session = Umsatz / Sitzungen
    if revenue_col_agg and sessions_col_agg:
        aggregated['Revenue per Session (â‚¬)'] = (
            (aggregated[revenue_col_agg] / aggregated[sessions_col_agg].replace(0, np.nan))
            .fillna(0)
            .replace([np.inf, -np.inf], 0)
        )
    
    return aggregated

def get_top_flop_asins(df, traffic_type='normal'):
    """Identifiziert Top- und Flop-ASINs basierend auf Umsatz"""
    if traffic_type == 'B2B':
        units_col = find_column(df, ['Bestellte Einheiten â€“ B2B', 'Bestellte Einheiten - B2B'])
        revenue_col = find_column(df, ['Bestellsumme â€“ B2B', 'Bestellsumme - B2B'])
        views_col = find_column(df, ['Seitenaufrufe â€“ Summe â€“ B2B', 'Seitenaufrufe - Summe - B2B'])
        sessions_col = find_column(df, ['Sitzungen â€“ Summe â€“ B2B', 'Sitzungen - Summe - B2B'])
        orders_col = find_column(df, ['Zahl der Bestellposten â€“ B2B', 'Zahl der Bestellposten - B2B'])
    else:
        units_col = find_column(df, ['Bestellte Einheiten'])
        revenue_col = find_column(df, ['Durch bestellte Produkte erzielter Umsatz'])
        views_col = find_column(df, ['Seitenaufrufe â€“ Summe', 'Seitenaufrufe - Summe'])
        sessions_col = find_column(df, ['Sitzungen â€“ Summe', 'Sitzungen - Summe'])
        orders_col = find_column(df, ['Zahl der Bestellposten'])
    
    # Fallback falls Spalten nicht gefunden
    if not all([units_col, revenue_col, views_col, sessions_col, orders_col]):
        return None, None
    
    # Verwende untergeordnete ASINs
    asin_column = '(Untergeordnete) ASIN'
    if asin_column not in df.columns:
        asin_column = '(Ãœbergeordnete) ASIN'
    
    if asin_column not in df.columns:
        return None, None
    
    # Aggregiere nach ASIN
    asin_data = df.groupby(asin_column).agg({
        units_col: 'sum',
        revenue_col: 'sum',
        views_col: 'sum',
        sessions_col: 'sum',
        orders_col: 'sum'
    }).reset_index()
    
    # Berechne KPIs
    asin_data['Conversion Rate (%)'] = (
        (asin_data[units_col] / asin_data[sessions_col].replace(0, np.nan) * 100)
        .fillna(0)
        .replace([np.inf, -np.inf], 0)
    )
    asin_data['AOV (â‚¬)'] = (
        (asin_data[revenue_col] / asin_data[orders_col].replace(0, np.nan))
        .fillna(0)
        .replace([np.inf, -np.inf], 0)
    )
    asin_data['Revenue per Session (â‚¬)'] = (
        (asin_data[revenue_col] / asin_data[sessions_col].replace(0, np.nan))
        .fillna(0)
        .replace([np.inf, -np.inf], 0)
    )
    
    # Sortiere nach Umsatz (absteigend)
    asin_data = asin_data.sort_values(revenue_col, ascending=False)
    
    # Top ASIN (hÃ¶chster Umsatz)
    top_asins = asin_data.head(1).copy()
    top_asins.columns = ['ASIN', 'Einheiten', 'Umsatz', 'Seitenaufrufe', 'Sitzungen', 'Bestellungen', 'Conversion Rate (%)', 'AOV (â‚¬)', 'Revenue per Session (â‚¬)']
    
    # Flop ASIN (niedrigster Umsatz, aber > 0)
    # Filtere ASINs mit Umsatz > 0 und sortiere aufsteigend
    asin_data_with_revenue = asin_data[asin_data[revenue_col] > 0].copy()
    if len(asin_data_with_revenue) > 1:
        # Sortiere aufsteigend fÃ¼r Flop
        asin_data_with_revenue = asin_data_with_revenue.sort_values(revenue_col, ascending=True)
        flop_asins = asin_data_with_revenue.head(1).copy()
        flop_asins.columns = ['ASIN', 'Einheiten', 'Umsatz', 'Seitenaufrufe', 'Sitzungen', 'Bestellungen', 'Conversion Rate (%)', 'AOV (â‚¬)', 'Revenue per Session (â‚¬)']
    elif len(asin_data_with_revenue) == 1:
        # Nur ein ASIN mit Umsatz - das ist dann sowohl Top als auch Flop
        flop_asins = None
    else:
        flop_asins = None
    
    return top_asins, flop_asins

def generate_summary(current_data, previous_data, traffic_type='normal'):
    """Generiert eine Zusammenfassung der Ã„nderungen"""
    if previous_data is None or len(previous_data) == 0:
        return "Dies ist der erste Zeitraum. Keine Vergleichsdaten verfÃ¼gbar."
    
    current = current_data.iloc[-1] if len(current_data) > 0 else None
    previous = previous_data.iloc[-1] if len(previous_data) > 0 else None
    
    if current is None or previous is None:
        return "Nicht genÃ¼gend Daten fÃ¼r einen Vergleich verfÃ¼gbar."
    
    current_period = current['Zeitraum']
    previous_period = previous['Zeitraum']
    
    summary_parts = [f"**Vergleich zwischen {previous_period} und {current_period}:**\n\n"]
    
    # Bestellte Einheiten
    units_change = current['Bestellte Einheiten'] - previous['Bestellte Einheiten']
    units_pct = ((current['Bestellte Einheiten'] / previous['Bestellte Einheiten'] - 1) * 100) if previous['Bestellte Einheiten'] > 0 else 0
    if units_change > 0:
        summary_parts.append(f"âœ… Die bestellten Einheiten sind von {previous['Bestellte Einheiten']:.0f} auf {current['Bestellte Einheiten']:.0f} gestiegen (+{units_change:.0f} Einheiten, {units_pct:+.1f}%).")
    elif units_change < 0:
        summary_parts.append(f"âŒ Die bestellten Einheiten sind von {previous['Bestellte Einheiten']:.0f} auf {current['Bestellte Einheiten']:.0f} gesunken ({units_change:.0f} Einheiten, {units_pct:+.1f}%).")
    else:
        summary_parts.append(f"âž¡ï¸ Die bestellten Einheiten sind unverÃ¤ndert bei {current['Bestellte Einheiten']:.0f} Einheiten.")
    
    # Umsatz
    revenue_change = current['Umsatz'] - previous['Umsatz']
    revenue_pct = ((current['Umsatz'] / previous['Umsatz'] - 1) * 100) if previous['Umsatz'] > 0 else 0
    if revenue_change > 0:
        summary_parts.append(f"âœ… Der Umsatz ist von {previous['Umsatz']:,.2f} â‚¬ auf {current['Umsatz']:,.2f} â‚¬ gestiegen (+{revenue_change:,.2f} â‚¬, {revenue_pct:+.1f}%).")
    elif revenue_change < 0:
        summary_parts.append(f"âŒ Der Umsatz ist von {previous['Umsatz']:,.2f} â‚¬ auf {current['Umsatz']:,.2f} â‚¬ gesunken ({revenue_change:,.2f} â‚¬, {revenue_pct:+.1f}%).")
    else:
        summary_parts.append(f"âž¡ï¸ Der Umsatz ist unverÃ¤ndert bei {current['Umsatz']:,.2f} â‚¬.")
    
    # Seitenaufrufe (nur wenn verfÃ¼gbar)
    if 'Seitenaufrufe' in current and 'Seitenaufrufe' in previous:
        views_change = current['Seitenaufrufe'] - previous['Seitenaufrufe']
        views_pct = ((current['Seitenaufrufe'] / previous['Seitenaufrufe'] - 1) * 100) if previous['Seitenaufrufe'] > 0 else 0
        if views_change > 0:
            summary_parts.append(f"âœ… Die Seitenaufrufe sind von {previous['Seitenaufrufe']:.0f} auf {current['Seitenaufrufe']:.0f} gestiegen (+{views_change:.0f}, {views_pct:+.1f}%).")
        elif views_change < 0:
            summary_parts.append(f"âŒ Die Seitenaufrufe sind von {previous['Seitenaufrufe']:.0f} auf {current['Seitenaufrufe']:.0f} gesunken ({views_change:.0f}, {views_pct:+.1f}%).")
        else:
            summary_parts.append(f"âž¡ï¸ Die Seitenaufrufe sind unverÃ¤ndert bei {current['Seitenaufrufe']:.0f}.")
    elif 'Sitzungen' in current and 'Sitzungen' in previous:
        # Falls keine Seitenaufrufe, verwende Sitzungen
        sessions_change = current['Sitzungen'] - previous['Sitzungen']
        sessions_pct = ((current['Sitzungen'] / previous['Sitzungen'] - 1) * 100) if previous['Sitzungen'] > 0 else 0
        if sessions_change > 0:
            summary_parts.append(f"âœ… Die Sitzungen sind von {previous['Sitzungen']:.0f} auf {current['Sitzungen']:.0f} gestiegen (+{sessions_change:.0f}, {sessions_pct:+.1f}%).")
        elif sessions_change < 0:
            summary_parts.append(f"âŒ Die Sitzungen sind von {previous['Sitzungen']:.0f} auf {current['Sitzungen']:.0f} gesunken ({sessions_change:.0f}, {sessions_pct:+.1f}%).")
        else:
            summary_parts.append(f"âž¡ï¸ Die Sitzungen sind unverÃ¤ndert bei {current['Sitzungen']:.0f}.")
    
    # Conversion Rate
    if 'Conversion Rate (%)' in current and 'Conversion Rate (%)' in previous:
        cr_change = current['Conversion Rate (%)'] - previous['Conversion Rate (%)']
        if cr_change > 0:
            summary_parts.append(f"âœ… Die Conversion Rate ist von {previous['Conversion Rate (%)']:.2f}% auf {current['Conversion Rate (%)']:.2f}% gestiegen (+{cr_change:.2f} Prozentpunkte).")
        elif cr_change < 0:
            summary_parts.append(f"âŒ Die Conversion Rate ist von {previous['Conversion Rate (%)']:.2f}% auf {current['Conversion Rate (%)']:.2f}% gesunken ({cr_change:.2f} Prozentpunkte).")
        else:
            summary_parts.append(f"âž¡ï¸ Die Conversion Rate ist unverÃ¤ndert bei {current['Conversion Rate (%)']:.2f}%.")
    
    # AOV
    if 'AOV (â‚¬)' in current and 'AOV (â‚¬)' in previous:
        aov_change = current['AOV (â‚¬)'] - previous['AOV (â‚¬)']
        if aov_change > 0:
            summary_parts.append(f"âœ… Der Average Order Value ist von {previous['AOV (â‚¬)']:.2f} â‚¬ auf {current['AOV (â‚¬)']:.2f} â‚¬ gestiegen (+{aov_change:.2f} â‚¬).")
        elif aov_change < 0:
            summary_parts.append(f"âŒ Der Average Order Value ist von {previous['AOV (â‚¬)']:.2f} â‚¬ auf {current['AOV (â‚¬)']:.2f} â‚¬ gesunken ({aov_change:.2f} â‚¬).")
        else:
            summary_parts.append(f"âž¡ï¸ Der Average Order Value ist unverÃ¤ndert bei {current['AOV (â‚¬)']:.2f} â‚¬.")
    
    # Revenue per Session
    if 'Revenue per Session (â‚¬)' in current and 'Revenue per Session (â‚¬)' in previous:
        rps_change = current['Revenue per Session (â‚¬)'] - previous['Revenue per Session (â‚¬)']
        if rps_change > 0:
            summary_parts.append(f"âœ… Der Revenue per Session ist von {previous['Revenue per Session (â‚¬)']:.2f} â‚¬ auf {current['Revenue per Session (â‚¬)']:.2f} â‚¬ gestiegen (+{rps_change:.2f} â‚¬).")
        elif rps_change < 0:
            summary_parts.append(f"âŒ Der Revenue per Session ist von {previous['Revenue per Session (â‚¬)']:.2f} â‚¬ auf {current['Revenue per Session (â‚¬)']:.2f} â‚¬ gesunken ({rps_change:.2f} â‚¬).")
        else:
            summary_parts.append(f"âž¡ï¸ Der Revenue per Session ist unverÃ¤ndert bei {current['Revenue per Session (â‚¬)']:.2f} â‚¬.")
    
    return "\n\n".join(summary_parts)

# CSV-Upload
st.header("ðŸ“ Daten-Upload")
uploaded_files = st.file_uploader(
    "Lade eine oder mehrere CSV-Dateien hoch",
    type=['csv'],
    accept_multiple_files=True
)

if uploaded_files:
    # Lade und verarbeite alle Dateien
    all_dataframes = []
    for uploaded_file in uploaded_files:
        df = load_and_process_csv(uploaded_file, uploaded_file.name)
        if df is not None:
            all_dataframes.append(df)
    
    if all_dataframes:
        # Kombiniere alle DataFrames
        combined_df = pd.concat(all_dataframes, ignore_index=True)
        
        # Sortiere nach Zeitraum
        combined_df = combined_df.sort_values('Zeitraum')
        
        st.success(f"âœ… {len(all_dataframes)} Datei(en) erfolgreich geladen!")
        
        # Sidebar fÃ¼r Filter
        st.sidebar.header("ðŸ” Filter")
        
        # Traffic-Typ Auswahl
        traffic_type = st.sidebar.radio(
            "Traffic-Typ",
            ['Normal', 'B2B'],
            index=0
        )
        traffic_type_key = 'B2B' if traffic_type == 'B2B' else 'normal'
        
        # PrÃ¼fe ob es Account-Level oder ASIN-Level Reports sind
        is_account_level = combined_df['Report_Typ'].iloc[0] == 'Account-Level' if 'Report_Typ' in combined_df.columns else False
        
        # ASIN-Filter nur bei ASIN-Level Reports
        if not is_account_level:
            asin_column = '(Untergeordnete) ASIN'
            if asin_column not in combined_df.columns:
                # Fallback auf Ã¼bergeordnete ASINs falls Spalte nicht existiert
                asin_column = '(Ãœbergeordnete) ASIN'
            
            if asin_column in combined_df.columns:
                all_asins = combined_df[asin_column].unique().tolist()
                all_asins = [asin for asin in all_asins if pd.notna(asin) and str(asin).strip() != '']  # Entferne leere Werte
                all_asins.sort()
                
                selected_asins = st.sidebar.multiselect(
                    "ASINs filtern (leer = alle)",
                    all_asins,
                    default=[]
                )
                
                # Filtere Daten nach ASINs
                if selected_asins:
                    filtered_df = combined_df[combined_df[asin_column].isin(selected_asins)].copy()
                else:
                    filtered_df = combined_df.copy()
            else:
                filtered_df = combined_df.copy()
        else:
            # Account-Level: Keine ASIN-Filterung mÃ¶glich
            filtered_df = combined_df.copy()
            st.sidebar.info("â„¹ï¸ Account-Level Report: ASIN-Filterung nicht verfÃ¼gbar")
        
        # Hauptbereich
        st.header("ðŸ“ˆ KPI-Ãœbersicht")
        
        # DEBUG: Zeige alle verfÃ¼gbaren Spalten
        with st.expander("ðŸ” Debug: VerfÃ¼gbare Spalten anzeigen", expanded=False):
            st.write("**Alle Spalten im DataFrame:**")
            st.write(list(filtered_df.columns))
            st.write(f"\n**Anzahl Spalten:** {len(filtered_df.columns)}")
            
            # Zeige relevante Spalten
            st.write("\n**Relevante Spalten fÃ¼r aktuellen Traffic-Typ:**")
            if traffic_type_key == 'B2B':
                st.write("- Gesucht: 'Bestellte Einheiten â€“ B2B'")
                st.write("- Gesucht: 'Bestellsumme â€“ B2B'")
                st.write("- Gesucht: 'Seitenaufrufe â€“ Summe â€“ B2B'")
            else:
                st.write("- Gesucht: 'Bestellte Einheiten'")
                st.write("- Gesucht: 'Durch bestellte Produkte erzielter Umsatz'")
                st.write("- Gesucht: 'Seitenaufrufe â€“ Summe'")
            
            # Finde Ã¤hnliche Spalten
            st.write("\n**Ã„hnliche Spalten gefunden:**")
            all_cols = list(filtered_df.columns)
            search_terms = ['seitenaufrufe', 'sitzungen', 'summe', 'bestellte', 'einheiten', 'umsatz', 'b2b']
            for term in search_terms:
                matching = [col for col in all_cols if term.lower() in col.lower()]
                if matching:
                    st.write(f"- '{term}': {matching}")
        
        # Aggregiere Daten
        aggregated_data = aggregate_data(filtered_df, traffic_type_key, is_account_level=is_account_level)
        
        # PrÃ¼fe ob Daten auf Tagesebene sind
        # Versuche ZeitrÃ¤ume zu parsen und prÃ¼fe ob es Tagesdaten sind
        try:
            periods_as_dates = pd.to_datetime(aggregated_data['Zeitraum'], errors='coerce')
            valid_dates = periods_as_dates.dropna()
            if len(valid_dates) > 0:
                # PrÃ¼fe ob ZeitrÃ¤ume tÃ¤gliche Unterschiede haben
                date_diffs = valid_dates.diff().dropna()
                # Wenn die meisten Unterschiede 1 Tag sind, sind es Tagesdaten
                daily_diffs = (date_diffs == pd.Timedelta(days=1)).sum()
                is_daily_data = len(date_diffs) > 0 and (daily_diffs / len(date_diffs)) > 0.5
            else:
                is_daily_data = False
        except:
            is_daily_data = False
        
        # Aggregationsebene-Auswahl
        if is_daily_data:
            st.sidebar.subheader("ðŸ“… Aggregationsebene")
            aggregation_level = st.sidebar.radio(
                "Zeitraum-Aggregation",
                ['Woche', 'Monat', 'YTD'],
                index=0,
                help="WÃ¤hlen Sie, auf welcher Ebene die Daten angezeigt werden sollen"
            )
            
            # Konvertiere Auswahl zu Period-Key
            period_map = {'Woche': 'week', 'Monat': 'month', 'YTD': 'ytd'}
            period_key = period_map[aggregation_level]
        else:
            aggregation_level = None
            period_key = 'week'
        
        # Aggregiere Daten nach gewÃ¤hlter Ebene (vor Jahr-Filterung)
        if is_daily_data:
            aggregated_data = aggregate_by_period(aggregated_data, period=period_key)
        
        # Jahr-Auswahl (wenn mehrere Jahre vorhanden)
        if 'Zeitraum' in aggregated_data.columns:
            if period_key == 'ytd':
                # Bei YTD sind Jahre bereits im Zeitraum-String (z.B. "2024 (YTD)")
                # Extrahiere Jahre aus Zeitraum-Strings
                available_years = []
                for period_str in aggregated_data['Zeitraum'].unique():
                    year_match = re.search(r'(\d{4})\s*\(YTD\)', str(period_str))
                    if year_match:
                        available_years.append(int(year_match.group(1)))
                available_years = sorted(list(set(available_years)))
            else:
                # Extrahiere Jahre aus ZeitrÃ¤umen
                # Verwende Regex, um Jahre aus allen Zeitraum-Formaten zu extrahieren
                # (funktioniert fÃ¼r Datumsangaben, Wochen, Monate, etc.)
                available_years = []
                for period_str in aggregated_data['Zeitraum'].unique():
                    # Versuche Jahr aus verschiedenen Formaten zu extrahieren
                    year_match = re.search(r'(\d{4})', str(period_str))
                    if year_match:
                        available_years.append(int(year_match.group(1)))
                available_years = sorted(list(set(available_years)))
                
                # Erstelle Jahr_Extracted Spalte fÃ¼r Filterung
                aggregated_data['Jahr_Extracted'] = aggregated_data['Zeitraum'].str.extract(r'(\d{4})', expand=False).astype(float)
            
            if len(available_years) > 1:
                st.sidebar.subheader("ðŸ“† Jahr-Auswahl")
                selected_year = st.sidebar.selectbox(
                    "Jahr filtern",
                    ['Alle Jahre'] + [str(y) for y in available_years],
                    index=0,
                    help="WÃ¤hlen Sie ein Jahr, um nur Daten dieses Jahres anzuzeigen"
                )
                
                if selected_year != 'Alle Jahre':
                    year_filter = int(selected_year)
                    if period_key == 'ytd':
                        # Filtere nach Jahr im Zeitraum-String
                        aggregated_data = aggregated_data[
                            aggregated_data['Zeitraum'].str.contains(str(year_filter), na=False)
                        ].copy()
                    else:
                        # Filtere nach extrahiertem Jahr
                        if 'Jahr_Extracted' in aggregated_data.columns:
                            aggregated_data = aggregated_data[aggregated_data['Jahr_Extracted'] == year_filter].copy()
                        else:
                            # Fallback: Filtere nach String-Match
                            aggregated_data = aggregated_data[
                                aggregated_data['Zeitraum'].str.contains(str(year_filter), na=False)
                            ].copy()
            
            # Entferne temporÃ¤re Spalte
            if 'Jahr_Extracted' in aggregated_data.columns:
                aggregated_data = aggregated_data.drop(columns=['Jahr_Extracted'])
        
        # Erstelle numerische Zeitraum-IDs fÃ¼r die X-Achse
        aggregated_data = aggregated_data.copy()
        aggregated_data['Zeitraum_Nr'] = range(1, len(aggregated_data) + 1)
        
        # Statistiken (ganz oben)
        st.header("ðŸ“Š Statistiken")
        col1, col2, col3, col4, col5, col6 = st.columns(6)
        
        # Finde die tatsÃ¤chlichen Spaltennamen (mit flexibler Suche)
        units_col_stat = find_column(filtered_df, ['Bestellte Einheiten' if traffic_type_key == 'normal' else 'Bestellte Einheiten â€“ B2B', 'Bestellte Einheiten - B2B'])
        revenue_col_stat = find_column(filtered_df, ['Durch bestellte Produkte erzielter Umsatz' if traffic_type_key == 'normal' else 'Bestellsumme â€“ B2B', 'Bestellsumme - B2B'])
        views_col_stat = find_column(filtered_df, [
            'Seitenaufrufe â€“ Summe' if traffic_type_key == 'normal' else 'Seitenaufrufe â€“ Summe â€“ B2B',
            'Seitenaufrufe - Summe',
            'Sitzungen â€“ Summe',
            'Sitzungen - Summe',
            'Seitenaufrufe â€“ Summe â€“ B2B',
            'Seitenaufrufe - Summe - B2B'
        ])
        
        # Fallback falls Spalten nicht gefunden werden
        if units_col_stat is None:
            units_col_stat = 'Bestellte Einheiten' if traffic_type_key == 'normal' else 'Bestellte Einheiten â€“ B2B'
        if revenue_col_stat is None:
            revenue_col_stat = 'Durch bestellte Produkte erzielter Umsatz' if traffic_type_key == 'normal' else 'Bestellsumme â€“ B2B'
        if views_col_stat is None:
            views_col_stat = 'Seitenaufrufe â€“ Summe' if traffic_type_key == 'normal' else 'Seitenaufrufe â€“ Summe â€“ B2B'
        
        with col1:
            if units_col_stat and units_col_stat in filtered_df.columns:
                units_numeric = filtered_df[units_col_stat].apply(parse_numeric_value)
                total_units = units_numeric.sum()
            else:
                total_units = 0
            st.metric("Gesamt bestellte Einheiten", f"{total_units:,.0f}")
        
        with col2:
            if revenue_col_stat and revenue_col_stat in filtered_df.columns:
                revenue_numeric = filtered_df[revenue_col_stat].apply(parse_euro_value)
                total_revenue = revenue_numeric.sum()
            else:
                total_revenue = 0
            st.metric("Gesamtumsatz", f"{total_revenue:,.2f} â‚¬")
        
        with col3:
            # Seitenaufrufe oder Sitzungen
            if views_col_stat and views_col_stat in filtered_df.columns:
                # Konvertiere zu numerisch und berechne Summe
                views_numeric = filtered_df[views_col_stat].apply(parse_numeric_value)
                total_views = views_numeric.sum()
                if total_views > 0:
                    st.metric("Gesamt Seitenaufrufe", f"{total_views:,.0f}")
                elif 'Sitzungen â€“ Summe' in filtered_df.columns:
                    sessions_numeric = filtered_df['Sitzungen â€“ Summe'].apply(parse_numeric_value)
                    total_sessions = sessions_numeric.sum()
                    st.metric("Gesamt Sitzungen", f"{total_sessions:,.0f}")
                else:
                    st.metric("Gesamt Seitenaufrufe", "N/A")
            elif 'Sitzungen â€“ Summe' in filtered_df.columns:
                sessions_numeric = filtered_df['Sitzungen â€“ Summe'].apply(parse_numeric_value)
                total_sessions = sessions_numeric.sum()
                st.metric("Gesamt Sitzungen", f"{total_sessions:,.0f}")
            else:
                st.metric("Gesamt Seitenaufrufe", "N/A")
        
        with col4:
            asin_col_metric = '(Untergeordnete) ASIN' if '(Untergeordnete) ASIN' in filtered_df.columns else '(Ãœbergeordnete) ASIN'
            unique_asins = filtered_df[asin_col_metric].nunique() if asin_col_metric in filtered_df.columns else 0
            st.metric("Anzahl ASINs", f"{unique_asins}")
        
        with col5:
            # Durchschnittliche Conversion Rate
            avg_cr = aggregated_data['Conversion Rate (%)'].mean() if 'Conversion Rate (%)' in aggregated_data.columns else 0
            st.metric("Ã˜ Conversion Rate", f"{avg_cr:.2f}%")
        
        with col6:
            # Durchschnittlicher AOV
            avg_aov = aggregated_data['AOV (â‚¬)'].mean() if 'AOV (â‚¬)' in aggregated_data.columns else 0
            st.metric("Ã˜ AOV", f"{avg_aov:.2f} â‚¬")
        
        st.divider()
        
        # KPI-Ãœbersicht (Kombinierte Visualisierung)
        st.subheader("ðŸ“Š KPI-Ãœbersicht")
        
        # Bestimme den dritten Titel basierend auf verfÃ¼gbaren Daten
        if 'Seitenaufrufe' in aggregated_data.columns and aggregated_data['Seitenaufrufe'].sum() > 0:
            third_title = 'Seitenaufrufe'
        elif 'Sitzungen' in aggregated_data.columns:
            third_title = 'Sitzungen'
        else:
            third_title = 'Nicht verfÃ¼gbar'
        
        fig_combined = make_subplots(
            rows=1, cols=3,
            subplot_titles=('Bestellte Einheiten', 'Umsatz (â‚¬)', third_title),
            specs=[[{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}]]
        )
        
        fig_combined.add_trace(
            go.Bar(x=aggregated_data['Zeitraum_Nr'], y=aggregated_data['Bestellte Einheiten'], name='Einheiten'),
            row=1, col=1
        )
        
        fig_combined.add_trace(
            go.Bar(x=aggregated_data['Zeitraum_Nr'], y=aggregated_data['Umsatz'], name='Umsatz', marker_color='green'),
            row=1, col=2
        )
        
        # Seitenaufrufe oder Sitzungen fÃ¼r dritte Spalte
        if 'Seitenaufrufe' in aggregated_data.columns and aggregated_data['Seitenaufrufe'].sum() > 0:
            fig_combined.add_trace(
                go.Bar(x=aggregated_data['Zeitraum_Nr'], y=aggregated_data['Seitenaufrufe'], name='Seitenaufrufe', marker_color='blue'),
                row=1, col=3
            )
        elif 'Sitzungen' in aggregated_data.columns:
            fig_combined.add_trace(
                go.Bar(x=aggregated_data['Zeitraum_Nr'], y=aggregated_data['Sitzungen'], name='Sitzungen', marker_color='blue'),
                row=1, col=3
            )
        else:
            fig_combined.add_trace(
                go.Bar(x=aggregated_data['Zeitraum_Nr'], y=[0]*len(aggregated_data), name='Nicht verfÃ¼gbar', marker_color='gray'),
                row=1, col=3
            )
        
        fig_combined.update_layout(height=400, showlegend=False)
        fig_combined.update_xaxes(title_text='Zeitraum', tickmode='linear', tick0=1, dtick=1)
        st.plotly_chart(fig_combined, use_container_width=True)
        
        # Neue KPIs
        st.subheader("ðŸ“Š ZusÃ¤tzliche KPIs")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            fig_cr = px.line(
                aggregated_data,
                x='Zeitraum_Nr',
                y='Conversion Rate (%)',
                title=f'Conversion Rate ({traffic_type})',
                labels={'Conversion Rate (%)': 'Conversion Rate (%)', 'Zeitraum_Nr': 'Zeitraum'},
                markers=True
            )
            fig_cr.update_layout(height=300, xaxis=dict(tickmode='linear', tick0=1, dtick=1))
            fig_cr.update_xaxes(title_text='Zeitraum')
            fig_cr.update_traces(line_color='purple', marker_color='purple')
            st.plotly_chart(fig_cr, use_container_width=True)
        
        with col2:
            fig_aov = px.bar(
                aggregated_data,
                x='Zeitraum_Nr',
                y='AOV (â‚¬)',
                title=f'Average Order Value ({traffic_type})',
                labels={'AOV (â‚¬)': 'AOV (â‚¬)', 'Zeitraum_Nr': 'Zeitraum'}
            )
            fig_aov.update_layout(height=300, xaxis=dict(tickmode='linear', tick0=1, dtick=1))
            fig_aov.update_xaxes(title_text='Zeitraum')
            fig_aov.update_traces(marker_color='orange')
            st.plotly_chart(fig_aov, use_container_width=True)
        
        with col3:
            fig_rps = px.bar(
                aggregated_data,
                x='Zeitraum_Nr',
                y='Revenue per Session (â‚¬)',
                title=f'Revenue per Session ({traffic_type})',
                labels={'Revenue per Session (â‚¬)': 'Revenue/Session (â‚¬)', 'Zeitraum_Nr': 'Zeitraum'}
            )
            fig_rps.update_layout(height=300, xaxis=dict(tickmode='linear', tick0=1, dtick=1))
            fig_rps.update_xaxes(title_text='Zeitraum')
            fig_rps.update_traces(marker_color='teal')
            st.plotly_chart(fig_rps, use_container_width=True)
        
        # Mobile vs Browser Performance (nur wenn Daten verfÃ¼gbar)
        # PrÃ¼fe ob sowohl Mobile als auch Browser Daten vorhanden sind UND ob sie nicht alle 0 sind
        has_mobile_data = 'Mobile Sitzungen' in aggregated_data.columns
        has_browser_data = 'Browser Sitzungen' in aggregated_data.columns
        
        if has_mobile_data and has_browser_data:
            # PrÃ¼fe ob Daten vorhanden sind (nicht alle 0)
            mobile_sum = aggregated_data['Mobile Sitzungen'].sum() if has_mobile_data else 0
            browser_sum = aggregated_data['Browser Sitzungen'].sum() if has_browser_data else 0
            
            if mobile_sum > 0 or browser_sum > 0:
                st.subheader("ðŸ“± Mobile vs Browser Performance")
                
                # Bereite Daten fÃ¼r Mobile vs Browser vor
                mobile_browser_data = aggregated_data[['Zeitraum_Nr', 'Mobile Sitzungen', 'Browser Sitzungen']].copy()
                mobile_browser_data = mobile_browser_data.melt(
                    id_vars='Zeitraum_Nr',
                    value_vars=['Mobile Sitzungen', 'Browser Sitzungen'],
                    var_name='GerÃ¤t',
                    value_name='Sitzungen'
                )
                
                col1, col2 = st.columns(2)
                
                with col1:
                    fig_mobile_browser = px.bar(
                        mobile_browser_data,
                        x='Zeitraum_Nr',
                        y='Sitzungen',
                        color='GerÃ¤t',
                        title=f'Mobile vs Browser Sitzungen ({traffic_type})',
                        labels={'Sitzungen': 'Anzahl Sitzungen', 'Zeitraum_Nr': 'Zeitraum'},
                        color_discrete_map={'Mobile Sitzungen': '#1f77b4', 'Browser Sitzungen': '#ff7f0e'}
                    )
                    fig_mobile_browser.update_layout(height=350, xaxis=dict(tickmode='linear', tick0=1, dtick=1))
                    fig_mobile_browser.update_xaxes(title_text='Zeitraum')
                    st.plotly_chart(fig_mobile_browser, use_container_width=True)
                
                with col2:
                    # Berechne Mobile vs Browser Anteil
                    mobile_browser_pct = aggregated_data.copy()
                    total_sessions = mobile_browser_pct['Mobile Sitzungen'] + mobile_browser_pct['Browser Sitzungen']
                    mobile_browser_pct['Mobile %'] = (mobile_browser_pct['Mobile Sitzungen'] / total_sessions.replace(0, np.nan) * 100).fillna(0)
                    mobile_browser_pct['Browser %'] = (mobile_browser_pct['Browser Sitzungen'] / total_sessions.replace(0, np.nan) * 100).fillna(0)
                    
                    mobile_browser_pct_data = mobile_browser_pct[['Zeitraum_Nr', 'Mobile %', 'Browser %']].melt(
                        id_vars='Zeitraum_Nr',
                        value_vars=['Mobile %', 'Browser %'],
                        var_name='GerÃ¤t',
                        value_name='Anteil (%)'
                    )
                    
                    fig_mobile_browser_pct = px.bar(
                        mobile_browser_pct_data,
                        x='Zeitraum_Nr',
                        y='Anteil (%)',
                        color='GerÃ¤t',
                        title=f'Mobile vs Browser Anteil ({traffic_type})',
                        labels={'Anteil (%)': 'Anteil (%)', 'Zeitraum_Nr': 'Zeitraum'},
                        color_discrete_map={'Mobile %': '#1f77b4', 'Browser %': '#ff7f0e'}
                    )
                    fig_mobile_browser_pct.update_layout(height=350, xaxis=dict(tickmode='linear', tick0=1, dtick=1), barmode='stack')
                    fig_mobile_browser_pct.update_xaxes(title_text='Zeitraum')
                    st.plotly_chart(fig_mobile_browser_pct, use_container_width=True)
            # Wenn keine Daten vorhanden, wird die Sektion einfach nicht angezeigt
        
        # Zusammenfassung
        st.header("ðŸ“ Zusammenfassung")
        
        if len(aggregated_data) > 1:
            # Zeitraum-Auswahl fÃ¼r Vergleich
            available_periods = aggregated_data['Zeitraum'].unique().tolist()
            available_periods.sort()
            
            col1, col2 = st.columns(2)
            
            with col1:
                previous_period = st.selectbox(
                    "Vergleichszeitraum (von)",
                    available_periods,
                    index=len(available_periods) - 2 if len(available_periods) > 1 else 0,
                    help="WÃ¤hlen Sie den ersten Zeitraum fÃ¼r den Vergleich"
                )
            
            with col2:
                current_period = st.selectbox(
                    "Aktueller Zeitraum (zu)",
                    available_periods,
                    index=len(available_periods) - 1,
                    help="WÃ¤hlen Sie den zweiten Zeitraum fÃ¼r den Vergleich"
                )
            
            # Filtere Daten fÃ¼r die ausgewÃ¤hlten ZeitrÃ¤ume
            previous_data = aggregated_data[aggregated_data['Zeitraum'] == previous_period].copy()
            current_data = aggregated_data[aggregated_data['Zeitraum'] == current_period].copy()
            
            if len(previous_data) > 0 and len(current_data) > 0:
                summary = generate_summary(current_data, previous_data, traffic_type_key)
            else:
                summary = "Fehler beim Laden der ZeitrÃ¤ume. Bitte wÃ¤hlen Sie andere ZeitrÃ¤ume aus."
        else:
            summary = "Nur ein Zeitraum verfÃ¼gbar. Lade weitere Dateien hoch, um Vergleiche zu sehen."
        
        st.markdown(summary)
        
        # Top- und Flop-ASINs (nur bei ASIN-Level Reports)
        if not is_account_level:
            st.subheader("ðŸ† Top- und Flop-ASINs")
            
            # Verwende den aktuellsten Zeitraum fÃ¼r Top/Flop Analyse
            latest_period = aggregated_data['Zeitraum'].iloc[-1] if len(aggregated_data) > 0 else None
            if latest_period:
                latest_df = filtered_df[filtered_df['Zeitraum'] == latest_period].copy()
            else:
                latest_df = filtered_df.copy()
            
            top_asins, flop_asins = get_top_flop_asins(latest_df, traffic_type_key)
            
            if top_asins is not None and len(top_asins) > 0:
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("### ðŸŸ¢ Top ASIN (nach Umsatz)")
                    row = top_asins.iloc[0]
                    with st.container():
                        st.markdown(f"**{row['ASIN']}**")
                        col_a, col_b = st.columns(2)
                        with col_a:
                            st.metric("Umsatz", f"{row['Umsatz']:,.2f} â‚¬")
                            st.metric("Einheiten", f"{row['Einheiten']:.0f}")
                        with col_b:
                            st.metric("Conversion Rate", f"{row['Conversion Rate (%)']:.2f}%")
                            st.metric("AOV", f"{row['AOV (â‚¬)']:.2f} â‚¬")
                        st.caption(f"Revenue/Session: {row['Revenue per Session (â‚¬)']:.2f} â‚¬ | Sitzungen: {row['Sitzungen']:.0f} | Seitenaufrufe: {row['Seitenaufrufe']:.0f}")
                
                with col2:
                    if flop_asins is not None and len(flop_asins) > 0:
                        st.markdown("### ðŸ”´ Flop ASIN (nach Umsatz)")
                        row = flop_asins.iloc[0]
                        with st.container():
                            st.markdown(f"**{row['ASIN']}**")
                            col_a, col_b = st.columns(2)
                            with col_a:
                                st.metric("Umsatz", f"{row['Umsatz']:,.2f} â‚¬")
                                st.metric("Einheiten", f"{row['Einheiten']:.0f}")
                            with col_b:
                                st.metric("Conversion Rate", f"{row['Conversion Rate (%)']:.2f}%")
                                st.metric("AOV", f"{row['AOV (â‚¬)']:.2f} â‚¬")
                            st.caption(f"Revenue/Session: {row['Revenue per Session (â‚¬)']:.2f} â‚¬ | Sitzungen: {row['Sitzungen']:.0f} | Seitenaufrufe: {row['Seitenaufrufe']:.0f}")
                    else:
                        st.markdown("### ðŸ”´ Flop ASIN")
                        st.info("Keine Flop-ASIN verfÃ¼gbar (nur ein ASIN mit Umsatz vorhanden oder alle ASINs haben keinen Umsatz).")
            else:
                st.info("Top- und Flop-ASINs konnten nicht berechnet werden. Bitte Ã¼berprÃ¼fe die Daten.")
        else:
            st.info("â„¹ï¸ Account-Level Report: Top- und Flop-ASINs sind nicht verfÃ¼gbar (Daten sind bereits auf Account-Ebene aggregiert).")
        
        # Detaillierte Tabelle
        st.header("ðŸ“‹ Detaillierte Daten")
        
        # Finde die tatsÃ¤chlichen Spaltennamen fÃ¼r die Anzeige
        units_col_display = find_column(filtered_df, ['Bestellte Einheiten' if traffic_type_key == 'normal' else 'Bestellte Einheiten â€“ B2B'])
        revenue_col_display = find_column(filtered_df, ['Durch bestellte Produkte erzielter Umsatz' if traffic_type_key == 'normal' else 'Bestellsumme â€“ B2B'])
        views_col_display = find_column(filtered_df, [
            'Seitenaufrufe â€“ Summe' if traffic_type_key == 'normal' else 'Seitenaufrufe â€“ Summe â€“ B2B',
            'Sitzungen â€“ Summe',
            'Sitzungen - Summe'
        ])
        
        display_columns = ['Zeitraum']
        
        # FÃ¼ge ASIN-Spalten nur hinzu, wenn vorhanden (nicht bei Account-Level)
        if '(Ãœbergeordnete) ASIN' in filtered_df.columns:
            display_columns.append('(Ãœbergeordnete) ASIN')
        if '(Untergeordnete) ASIN' in filtered_df.columns:
            display_columns.append('(Untergeordnete) ASIN')
        if 'Titel' in filtered_df.columns:
            display_columns.append('Titel')
        
        # FÃ¼ge dynamisch gefundene Spalten hinzu
        if units_col_display:
            display_columns.append(units_col_display)
        if revenue_col_display:
            display_columns.append(revenue_col_display)
        if views_col_display:
            display_columns.append(views_col_display)
        
        available_columns = [col for col in display_columns if col in filtered_df.columns]
        st.dataframe(
            filtered_df[available_columns],
            use_container_width=True,
            height=400
        )
    else:
        st.error("Keine Daten konnten geladen werden. Bitte Ã¼berprÃ¼fe die CSV-Dateien.")
else:
    st.info("ðŸ‘† Bitte lade eine oder mehrere CSV-Dateien hoch, um zu beginnen.")

